---
title: "Hypertuning"
author: "Rithin"
date: "2/4/2020"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r}
library(keras)
library(tensorflow)
library(tidyverse)
library(cowplot)

imdb <- dataset_imdb(num_words = 10000)
c(c(train_data, train_labels), c(test_data, test_labels)) %<-% imdb
```

## preparing the data
```{r}
vectorize_sequences <- function(sequences, dimension = 10000) {
  # Create an all-zero matrix of shape (len(sequences), dimension)
  results <- matrix(0, nrow = length(sequences), ncol = dimension)
  for (i in 1:length(sequences))
    # Sets specific indices of results[i] to 1s
    results[i, sequences[[i]]] <- 1
  results
}

# Our vectorized training data
x_train <- vectorize_sequences(train_data)
# Our vectorized test data
x_test <- vectorize_sequences(test_data)
# Our vectorized labels
y_train <- as.numeric(train_labels)
y_test <- as.numeric(test_labels)
val_indices <- 1:10000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]
```

## hidden layer
```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 16, activation = "tanh", input_shape = c(10000)) %>%
  layer_dense(units = 1, activation = "sigmoid")

val_indices <- 1:10000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]

model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)

history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

str(history)
plot(history)
```
This approach overfits let us try another with regularization and drop out


## hideen layer with regularization
```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 16,  kernel_regularizer = regularizer_l2(.0001),activation = "tanh", input_shape = c(10000)) %>%
  layer_dense(units = 1, activation = "sigmoid")

val_indices <- 1:10000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]

model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)

history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

str(history)
plot(history)
```
This network also appears to be overfit after one epoch

## hidden layer with regularization and drop out
```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 16,  kernel_regularizer = regularizer_l2(.0001),activation = "tanh", input_shape = c(10000)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")

val_indices <- 1:10000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]

model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)

history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

str(history)
plot(history)
```
This model overfits after one epoch, let us try by improvimg accuracy by adding more hiddden layers

## 3 hidden layers with out regularization and drop out

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, activation = "tanh", input_shape = c(10000)) %>%
  layer_dense(units = 32, activation = "tanh") %>%
  layer_dense(units = 16, activation = "tanh") %>%
  layer_dense(units = 1, activation = "sigmoid")

val_indices <- 1:10000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]

model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)

history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

str(history)
plot(history)
```
This layer also overfits aftyer one epoch let us improve accuracy by adding regularization 

## 3 hidden  layers with regularization
```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, kernel_regularizer = regularizer_l2(.0001), activation = "tanh", input_shape = c(10000)) %>%
  layer_dense(units = 32, kernel_regularizer = regularizer_l2(.0001), activation = "tanh") %>%
  layer_dense(units = 16, kernel_regularizer = regularizer_l2(.0001), activation = "tanh") %>%
  layer_dense(units = 1, activation = "sigmoid")

val_indices <- 1:10000

x_val <- x_train[val_indices,]
partial_x_train <- x_train[-val_indices,]

y_val <- y_train[val_indices]
partial_y_train <- y_train[-val_indices]

model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)

history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)

str(history)
plot(history)
```
This layer also overfits after one epoch. lets us try with same hidden layers with regularization and drop out

## 3 hidden layers with regularization and drop out

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 32, kernel_regularizer = regularizer_l1(.001), activation = "tanh", input_shape = c(10000)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, kernel_regularizer = regularizer_l1(.001), activation = "tanh") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, kernel_regularizer = regularizer_l1(.001), activation = "tanh") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")


model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)

history <- model %>% fit(
  partial_x_train,
  partial_y_train,
  epochs = 20,
  batch_size = 512,
  validation_data = list(x_val, y_val)
)
hist1 <- as.data.frame(history$metrics)
names(hist1) <- c("train-loss","train-accuracy","val_loss","val_accuracy")
hist1 <- hist1 %>% mutate(epochs = 1:n()) %>% gather("split","values",-epochs) %>% separate(split,c("split","metric")) %>% spread(metric,values)

 g1<- ggplot(hist1,aes(x=epochs,y=loss,color=split)) + geom_point() + geom_line() + theme_classic() + ggtitle("Validation loss") + theme(legend.position = "top",legend.justification = "left",legend.title = element_blank()) +scale_color_manual(values = c("green","red"))
 g2<- ggplot(hist1,aes(x=epochs,y=accuracy,color=split)) + geom_point(show.legend = F) + geom_line(show.legend = F) + theme_classic() + ggtitle("Validation Accuracy") + theme(legend.position = "top",legend.justification = "left",legend.title = element_blank()) + scale_color_manual(values = c("green","red")) 
plot_grid(g1,g2,nrow=2)
```
This network overfits after two epoch, Here validation loss is 10%  and accuracy is 88%.
This model with 3 hidden layers with regularization and drop out is the best model built.
we train a new model and evaluate on the test data

```{r}
model <- keras_model_sequential() %>% 
  layer_dense(units = 64, kernel_regularizer = regularizer_l1(.0001), activation = "tanh", input_shape = c(10000)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 32, kernel_regularizer = regularizer_l1(.0001), activation = "tanh") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, kernel_regularizer = regularizer_l1(.0001), activation = "tanh") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% compile(
  optimizer = "rmsprop",
  loss = "mse",
  metrics = c("accuracy")
)

model %>% fit(x_train, y_train, epochs = 8, batch_size = 512)
results <- model %>% evaluate(x_test, y_test)
```

## prediction on new data
```{r}
results
model %>% predict(x_test[1:10,])
```

